# REQ Enterprise Registry Scraper - Multi-Worker Architecture

This is the enhanced REQ scraper using your proven multi-worker architecture with Graphile Worker, proxy rotation, and stealth techniques.

## ðŸš€ Key Features

âœ… **Proven Architecture**: Based on your working Granby scraper
âœ… **Multi-Worker**: Concurrent processing with job queue
âœ… **Proxy Rotation**: Automatic rotation through proxy pool
âœ… **Stealth Mode**: Puppeteer-extra with stealth plugin
âœ… **Rate Limiting**: Smart delays and retry logic
âœ… **Complete REQ Workflow**: Quebec.ca â†’ REQ portal â†’ search â†’ Consulter â†’ details

## ðŸ›  Setup Instructions

### 1. Database Setup
```bash
# Copy your database credentials
cp env-example .env
# Edit .env with your PostgreSQL connection string

# Setup database schema
npm run setup
```

### 2. Install Dependencies
```bash
npm install -f package-worker.json
```

### 3. Configure Proxy Pool
Add more proxy IPs to `proxy-list.txt`:
```
http://user1:pass1@proxy1.rayobyte.com:8000
http://user2:pass2@proxy2.rayobyte.com:8000
http://user3:pass3@proxy3.rayobyte.com:8000
```

### 4. Queue Companies
```bash
npm run seed
```

### 5. Start Workers
```bash
# Start with 3 concurrent workers
npm run worker
```

## ðŸ“Š How It Works

1. **Job Queue**: Each company becomes a job in PostgreSQL queue
2. **Worker Pool**: Multiple workers process jobs concurrently
3. **Proxy Rotation**: Each job uses next proxy in rotation
4. **REQ Workflow**:
   - Navigate to Quebec.ca landing page
   - Click "AccÃ©der au service" â†’ REQ portal
   - Fill company name in `#Objet` input
   - Check terms checkbox
   - Submit search
   - Wait for results with `span:has-text("Consulter")` buttons
   - Find matching company in `<h4>` tags
   - Click corresponding Consulter button
   - Extract company details from detail page
   - Save to database

## ðŸŽ¯ Advantages Over Single-Process

- **Scale**: Process multiple companies simultaneously
- **Resilience**: Failed jobs auto-retry with different proxies
- **Efficiency**: No waiting for single proxy cooldown
- **Monitoring**: Database tracks all attempts and results
- **Restart**: Workers can be restarted without losing progress

## ðŸ“ˆ Expected Results

With multiple proxies and workers, this should successfully scrape your entire `entreprises.csv` list by distributing load across different IPs and avoiding the aggressive rate limiting we encountered with single-proxy approach.

## ðŸ”§ Scaling Up

To handle more companies faster:
1. Add more proxy IPs to `proxy-list.txt`
2. Increase `concurrency` in `main-worker.js`
3. Run multiple worker processes on different servers
4. Each worker automatically rotates through different proxies

---

**Status**: Ready for production testing with your proven multi-worker architecture! ðŸŽ‰